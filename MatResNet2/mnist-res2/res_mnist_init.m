function net = res_mnist_init(varargin)

net = dagnn.DagNN();
net.meta.inputSize = [32 32 3] ; 
net.meta.trainOpts.weightDecay = 0.0001 ; 
net.meta.trainOpts.momentum = 0.9; 
net.meta.trainOpts.batchSize = 100 ; 
net.meta.trainOpts.learningRate = ...
    [0.01*ones(1,80) 0.001*ones(1,80) 0.0001*ones(1,40)];
net.meta.trainOpts.numEpochs = numel(net.meta.trainOpts.learningRate);

net = addResGroup(net,'16',[28,28,1],[14,14,16],3);


end



function net = addResGroup(net, groupIndex, inputDims, outputDims, n,inputName,outputName)
    stride = 2;
    inputChannels = inputDims(3);
    outputChannels = outputDims(3);
    layer_index = [groupIndex,1,1];
    add_bn_layer(net,struct(...
        'name',sprintf('BN %s',mat2str(layer_index)),...
        'inputName',inputName,...
        'outputName',sprintf('x_%s',mat2str(layer_index))));
    
    prev_layer_index = layer_index;
    layer_index = [groupIndex,1,2];
    add_relu_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index)),...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'channels',inputChannels));
    
    prev_layer_index = layer_index;
    layer_index = [groupIndex,1,3];
    add_conv_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index))),...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'size',[3,3,inputChannels,outputChannels],...
        'pad',1,...
        'stride',stride);
    
    layer_index = [groupIndex,1,4];
    add_bn_layer(net,struct(...
        'name',sprintf('BN %s',mat2str(layer_index)),...
        'inputName',inputName,...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'channels',outputChannels));
    
    prev_layer_index = layer_index;
    layer_index = [groupIndex,1,5];
    add_relu_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index))),...
        'outputName',sprintf('x_%s',mat2str(layer_index)));
    
    prev_layer_index = layer_index;
    layer_index = [groupIndex,1,6];
    add_conv_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index))),...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'size',[3,3,outputChannels,outputChannels],...
        'pad',1,...
        'stride',1);
    
    prev_layer_index = layer_index;
    layer_index = [groupIndex,1];
    
    add_fc_layer(net,struct(...
        'name','Skipp fc',...
        'inputName',inputName,...
        'outputName',sprintf('xp_%s',mat2str(layer_index)),...
        'inputSize',inputDims,...
        'outputSize',outputDims));
    
    net.addLayer(sprintf('Add %s',mat2str(layer_index)),dagnn.Sum(),...
        {sprintf('xp_%s',mat2str(layer_index)),sprintf('x_%s',mat2str(prev_layer_index))},...
        sprintf('x_%s',mat2str(layer_index)));
    
    % ----------------------------------------------
    
    for i = 2:n
        resUnitName = sprintf('%s,%d', name,i);
        resLayers = resUnit.new(resUnitName);
        net.layers{end+1} = resLayers{1};
        net.layers{end+1} = newBnorm(sprintf('BN 1 - %s',resUnitName),outputChannels);
        net.layers{end+1} = struct(...
                    'name', sprintf('relu 1 - %s',resUnitName),...
                    'type', 'relu') ;
        net.layers{end+1} = struct( ...
                    'name',  sprintf('conv 1 - %s',resUnitName),...
                    'type', 'conv',...
                    'learningrate', [1 1 0.1],...
                    'weights', {xavier(3,3,outputChannels,outputChannels)},...
                    'pad', 1);

        net.layers{end+1} = newBnorm(sprintf('BN 2 - %s',resUnitName),outputChannels);
        net.layers{end+1} = struct(...
                    'name', sprintf('relu 2 - %s',resUnitName),...
                    'type', 'relu') ;
        net.layers{end+1} = struct( ...
                    'name', sprintf('conv 2 - %s',resUnitName),...
                    'type', 'conv',...
                    'learningrate', [1 1 0.1],...
                    'weights', {xavier(3,3,outputChannels,outputChannels)},...
                    'pad', 1);
        net.layers{end+1} = resLayers{2};
    end
end

function outputName = add_res_unit(opts)
%Opts must contains
%   inputName :: string
%   groupIndex :: int
%   resUnitIndex :: int
%   inputSize :: [int] length 3
%   outputSize :: [int] length 3
%   

    stride = 2;
    inputChannels = opts.inputSize(3);
    outputChannels = opts.outputSize(3);
    layer_index = [opts.groupIndex,opts.resUnitIndex,1];
    add_bn_layer(net,struct(...
        'name',sprintf('BN %s',mat2str(layer_index)),...
        'inputName',inputName,...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'channels',inputChannels));
    
    prev_layer_index = layer_index;
    layer_index(3) = 2;
    add_relu_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index)),...
        'outputName',sprintf('x_%s',mat2str(layer_index))));
    
    prev_layer_index = layer_index;
    layer_index(3) = 3;
    add_conv_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index))),...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'size',[3,3,inputChannels,outputChannels],...
        'pad',1,...
        'stride',stride);
    
    prev_layer_index = layer_index;
    layer_index(3) = 4;
    add_bn_layer(net,struct(...
        'name',sprintf('BN %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index)),...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'channels',outputChannels));
    
    prev_layer_index = layer_index;
    layer_index(3) = 5;
    add_relu_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index))),...
        'outputName',sprintf('x_%s',mat2str(layer_index)));
    
    prev_layer_index = layer_index;
    layer_index(3) = 6;
    add_conv_layer(net,struct(...
        'name',sprintf('relu %s',mat2str(layer_index)),...
        'inputName',sprintf('x_%s',mat2str(prev_layer_index))),...
        'outputName',sprintf('x_%s',mat2str(layer_index)),...
        'size',[3,3,outputChannels,outputChannels],...
        'pad',1,...
        'stride',1);
    
    prev_layer_index = layer_index;
    layer_index = [groupIndex,1];
    outputName = sprintf('x_%s',mat2str(layer_index));
    if opts.inputSize == opts.outputSize
        net.addLayer(sprintf('Add %s',mat2str(layer_index)),dagnn.Sum(),...
            {opts.inputName,sprintf('x_%s',mat2str(prev_layer_index))},...
            outputName);
    else
        add_fc_layer(net,struct(...
            'name','Skipp fc',...
            'inputName',inputName,...
            'outputName',sprintf('xp_%s',mat2str(layer_index)),...
            'inputSize',inputDims,...
            'outputSize',outputDims));

        net.addLayer(sprintf('Add %s',mat2str(layer_index)),dagnn.Sum(),...
            {sprintf('xp_%s',mat2str(layer_index)),sprintf('x_%s',mat2str(prev_layer_index))},...
            outputName);
    end
    
end

function add_conv_layer(net,opts)
    %Opts must contain the fields
    %   name :: string
    %   inputName :: string
    %   outputName :: string
    %   size :: [int] , length 4.
    %   stride :: int
    %   pad :: int
    block = dagnn.Conv(...
        'size',opts.size,...
        'hasBias',true,...
        'stride',opts.stride,...
        'pad',opts.padding);
    parameter_names = {[opts.name '_f'] [opts.name '_b']};
    net.addLayer(opts.name,block,opts.inputName,opts.outputName,parameter_names);
    net.params(net.getParamIndex(parameter_names(2))).weightDecay = 0; % Why set this to 0?
end

function add_bn_layer(net,opts)
    %Opts must contain the fields
    %   name :: string
    %   inputName :: string
    %   outputName :: string
    %   channels :: int
    %Opts may contain the fields
    %   learningRate :: double
    block = dagnn.BatchNorm('numChannels',opts.channels);
    parameter_names = {[opts.name '_g'], [opts.name '_b'], [opts.name '_m']};
    net.addLayer(opts.name,block,opts.inputName,opts.outputName,parameter_names);
    parameter_indexes = net.getParamIndex(parameter_names);
    net.params(parameter_indexes(1)).weightDecay = 0;
    net.params(parameter_indexes(2)).weightDecay = 0;
    if ~isfield('learningRate',opts)
        lr = .1;
    else
        lr = opts.learningRate;
    end
    net.params(pidx(3)).learningRate = lr;
    net.params(pidx(3)).trainMethod = 'average';
end

function add_relu_layer(net,opts)
    %Opts must contain
    %   name :: string
    %   inputName :: string
    %   outputName :: string
    %Opts may contain the fields
    %   leak
    if isField(opts,'leak')
        leak = opts.leak;
    else
        leak = 0;
    end
    block = dagnn.ReLU('leak',leak);
    net.addLayer(opts.name,block,opts.inputName,opts.outputName);
end

function add_fc_layer(net,opts)
    block = dagnn.Fc(...
        'hasBias',true,...
        'inSize',opts.inputSize,...
        'outSize',opts.outputSize);
    net.addLayer(opts.name,block,opts.inputName,opts.outputName);
end
